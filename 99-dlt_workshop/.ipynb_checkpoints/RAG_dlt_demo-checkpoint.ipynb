{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33774b8-ec57-49e9-adc6-c6a4f3c68d10",
   "metadata": {},
   "source": [
    "# Intro dlt -> LanceDB loading example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcfe33-f610-472c-97b8-ba9a5b98366c",
   "metadata": {},
   "source": [
    "https://lu.ma/cnpdoc5n\n",
    "\n",
    "If you want to play around with this notebook and make edits in future, we highly recommend making a copy since the link is view only! Also make sure you're signed in with your Google account to be able to add secrets.\n",
    "\n",
    "Before going into a more complex example, we will go through a simple example of how to load the course Q&A data into LanceDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca6c66-3b85-4655-83c9-f171a7cd60b9",
   "metadata": {},
   "source": [
    "\n",
    "## Install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a08df-63fd-429c-a98d-92e5ca00d421",
   "metadata": {},
   "source": [
    "To create a json -> lancedb pipeline, we need to install:\n",
    "\n",
    "dlt with lancedb extras\n",
    "sentence-transformers: we need to use an embedding model to vectorize and store data inside LanceDB. For this we choose the open-source model \"sentence-transformers/all-MiniLM-L6-v2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb99ceb-58f3-476f-a270-06c0faadedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dlt[lancedb]==0.5.1a0\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656350d-8dd2-475e-8cf6-93e04ae1301c",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff29ba0-75c0-4296-ac55-ed4feb6beb26",
   "metadata": {},
   "source": [
    "We'll first load the data just into LanceDB, without embedding it. LanceDB stores both the data and the embeddings, and can also embed data and queries on the fly.\n",
    "\n",
    "Some definitions:\n",
    "\n",
    "A dlt source is a grouping of resources (e.g. all your data from Hubspot)\n",
    "A dlt resource is a function that yields data (e.g. a function yielding all your Hubspot companies)\n",
    "A dlt pipeline is how you ingest your data\n",
    "Loading the data consists of a few steps:\n",
    "\n",
    "Use the requests library to get the data\n",
    "Define a dlt resource that yields the individual documents\n",
    "Create a dlt pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b86b8b-0184-4e4f-a4bf-f5b7b818b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3070773a-3b6e-477c-b7f2-7ed49bd55661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline from_json load step completed in 0.21 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x7e1f5a95ae00> location to store data\n",
      "Load package 1721574463.8597133 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import dlt\n",
    "\n",
    "qa_dataset = requests.get(\"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1\").json()\n",
    "\n",
    "@dlt.resource\n",
    "def qa_documents():\n",
    "  for course in qa_dataset:\n",
    "    yield course[\"documents\"]\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json\", destination=\"lancedb\", dataset_name=\"qanda\")\n",
    "\n",
    "load_info = pipeline.run(qa_documents, table_name=\"documents\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb922bd-be41-401f-abbe-818730a0e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___documents']\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13d2df5-9655-4b44-90e0-95229b907e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6faff58b-fdd4-5edd-97cf-fcc9c4367621</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721269342.5232055</td>\n",
       "      <td>maZaihIsaJYCCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4e67bd26-7103-58cf-8707-b58c6ce536a0</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721269342.5232055</td>\n",
       "      <td>xYWV3FxNDWxzkA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0333379e-89f6-5efe-bfdb-23f6566a5256</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721269342.5232055</td>\n",
       "      <td>spR+2eQAyFJsbQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fe5852db-0597-5a3c-9850-1611e3c8e5ff</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721269342.5232055</td>\n",
       "      <td>Wj0KiQo1p1xoSw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b27814dd-0c6a-50a7-a3b9-cdd872b38ccb</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721269342.5232055</td>\n",
       "      <td>C+M5yqHlAqnb2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>a7d9d187-771c-5ee2-a348-107891b9bd84</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721574463.8597133</td>\n",
       "      <td>n3l1+oOaCMM/RA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>c172c2d6-4e20-5ed1-ba6f-95734fe2752a</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721574463.8597133</td>\n",
       "      <td>ykyWC3tllCOxaQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>2ee4b228-d251-5fb4-8d96-7daf4c6eb350</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721574463.8597133</td>\n",
       "      <td>JfQZngdC+nlp2Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>b17d2c40-ced6-5311-8ea5-4c1b390e411b</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721574463.8597133</td>\n",
       "      <td>Pmz/WUZ9NjKulA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>7fceaaaa-f86f-5280-a442-82b1351383d1</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721574463.8597133</td>\n",
       "      <td>yMs0KYwdJ0IrGQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1896 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id__  \\\n",
       "0     6faff58b-fdd4-5edd-97cf-fcc9c4367621   \n",
       "1     4e67bd26-7103-58cf-8707-b58c6ce536a0   \n",
       "2     0333379e-89f6-5efe-bfdb-23f6566a5256   \n",
       "3     fe5852db-0597-5a3c-9850-1611e3c8e5ff   \n",
       "4     b27814dd-0c6a-50a7-a3b9-cdd872b38ccb   \n",
       "...                                    ...   \n",
       "1891  a7d9d187-771c-5ee2-a348-107891b9bd84   \n",
       "1892  c172c2d6-4e20-5ed1-ba6f-95734fe2752a   \n",
       "1893  2ee4b228-d251-5fb4-8d96-7daf4c6eb350   \n",
       "1894  b17d2c40-ced6-5311-8ea5-4c1b390e411b   \n",
       "1895  7fceaaaa-f86f-5280-a442-82b1351383d1   \n",
       "\n",
       "                                                   text  \\\n",
       "0     The purpose of this document is to capture fre...   \n",
       "1     GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2     Yes, even if you don't register, you're still ...   \n",
       "3     You don't need it. You're accepted. You can al...   \n",
       "4     You can start by installing and setting up all...   \n",
       "...                                                 ...   \n",
       "1891  Problem description\\nThis is the step in the c...   \n",
       "1892  Problem description\\nWhen a docker-compose fil...   \n",
       "1893  Problem description\\nIf you are having problem...   \n",
       "1894  Problem description\\nPre-commit command was fa...   \n",
       "1895  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                               section  \\\n",
       "0     General course-related questions   \n",
       "1     General course-related questions   \n",
       "2     General course-related questions   \n",
       "3     General course-related questions   \n",
       "4     General course-related questions   \n",
       "...                                ...   \n",
       "1891          Module 6: Best practices   \n",
       "1892          Module 6: Best practices   \n",
       "1893          Module 6: Best practices   \n",
       "1894          Module 6: Best practices   \n",
       "1895          Module 6: Best practices   \n",
       "\n",
       "                                               question        _dlt_load_id  \\\n",
       "0                  Course - When will the course start?  1721269342.5232055   \n",
       "1     Course - What are the prerequisites for this c...  1721269342.5232055   \n",
       "2     Course - Can I still join the course after the...  1721269342.5232055   \n",
       "3     Course - I have registered for the Data Engine...  1721269342.5232055   \n",
       "4      Course - What can I do before the course starts?  1721269342.5232055   \n",
       "...                                                 ...                 ...   \n",
       "1891  Github actions: Permission denied error when e...  1721574463.8597133   \n",
       "1892  Managing Multiple Docker Containers with docke...  1721574463.8597133   \n",
       "1893           AWS regions need to match docker-compose  1721574463.8597133   \n",
       "1894                                   Isort Pre-commit  1721574463.8597133   \n",
       "1895  How to destroy infrastructure created via GitH...  1721574463.8597133   \n",
       "\n",
       "             _dlt_id  \n",
       "0     maZaihIsaJYCCQ  \n",
       "1     xYWV3FxNDWxzkA  \n",
       "2     spR+2eQAyFJsbQ  \n",
       "3     Wj0KiQo1p1xoSw  \n",
       "4     C+M5yqHlAqnb2g  \n",
       "...              ...  \n",
       "1891  n3l1+oOaCMM/RA  \n",
       "1892  ykyWC3tllCOxaQ  \n",
       "1893  JfQZngdC+nlp2Q  \n",
       "1894  Pmz/WUZ9NjKulA  \n",
       "1895  yMs0KYwdJ0IrGQ  \n",
       "\n",
       "[1896 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table(\"qanda___documents\")\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae32b9-5b84-4b87-a9d2-36e7b5f904fa",
   "metadata": {},
   "source": [
    "## Load and embed the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a87b7f-6eeb-4210-85ac-b6c104489696",
   "metadata": {},
   "source": [
    "Now we load the same data again (into a new table), but embed it directly with the `lancedb_adapter`. This consists of the following steps:\n",
    "\n",
    "1. Define the embedding model to use via ENV variables\n",
    "2. Define a new pipeline to load the same data and embed the \"text\" and \"question\" columns with the `lancedb_adapter`\n",
    "\n",
    "You can use any embedding model, from open source to OpenAI. We've chosen the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) sentence transformer for speed and simplicty.\n",
    "\n",
    "Note: this pipeline runs slightly longer because it has to download the model and embed the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b6374-eeb5-4410-8b06-038e3d71df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dlt_pipeline_state\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, {'name': 'state', 'data_type': 'text', 'nullable': False}, {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "documents\n",
      "[{'name': 'text', 'x-lancedb-embed': True, 'data_type': 'text', 'nullable': True}, {'name': 'section', 'data_type': 'text', 'nullable': True}, {'name': 'question', 'x-lancedb-embed': True, 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "_dlt_version\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, {'name': 'schema', 'data_type': 'text', 'nullable': False}]\n",
      "_dlt_loads\n",
      "[{'name': 'load_id', 'data_type': 'text', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, {'name': 'status', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}]\n",
      "UPLOAD\n",
      "Pipeline from_json_embedded load step completed in 15.89 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda_embedded\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x7e1f51d53250> location to store data\n",
      "Load package 1721574737.3454354 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dlt.destinations.adapters import lancedb_adapter\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json_embedded\", destination=\"lancedb\", dataset_name=\"qanda_embedded\")\n",
    "\n",
    "load_info = pipeline.run(lancedb_adapter(qa_documents, embed=[\"text\", \"question\"]), table_name=\"documents\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095c205b-8d54-4633-a6ae-f6c8b417a9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___documents', 'qanda_embedded____dlt_loads', 'qanda_embedded____dlt_pipeline_state', 'qanda_embedded____dlt_version', 'qanda_embedded___dltSentinelTable', 'qanda_embedded___documents']\n"
     ]
    }
   ],
   "source": [
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6853e7a3-8559-4153-a5c8-a1f40d0f3925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>vector__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57b64c23-fc08-5929-a438-f37e3712a73d</td>\n",
       "      <td>[-0.00035094196, -0.062014297, -0.037999876, 0...</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>Xb2yoE2pdKyqAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74f4a51f-0227-5cd1-ac4e-5d2ed8d471e2</td>\n",
       "      <td>[0.020011412, -0.011535538, 0.013017209, -0.00...</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>fHsjSKqSkMPknA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ac330ebb-e555-5b5a-bee1-cc323af4e931</td>\n",
       "      <td>[0.014857555, -0.06664993, -0.013571247, 0.023...</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>4qQDkXTBMf+xrQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9351a81d-6c25-5223-b0f4-9ee047a620dc</td>\n",
       "      <td>[-0.023312032, -0.09461493, 0.056361612, -0.00...</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>vIF1WvHmGMbegw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fd61d778-c390-5d82-89e4-f77d0b27c8d9</td>\n",
       "      <td>[0.026537651, -0.01779666, 0.0021155947, 0.006...</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>tnTBbvaCCvvyEg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>06e95ae8-c6c8-51d6-bb0d-a7c3051f5a05</td>\n",
       "      <td>[0.016619362, -0.033603165, -0.093347155, -0.0...</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>zGKICl/U3GEZow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>38f191e7-9f59-518a-b060-87d670bddb38</td>\n",
       "      <td>[0.026872855, -0.0019949432, 0.008369081, -0.0...</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>QCcXun8zw6FQhg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>3b51d912-fcca-57f9-9060-214e604d66d9</td>\n",
       "      <td>[0.03513756, 0.056265566, 0.024428478, -0.0651...</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>kcfbZdKZSTP34w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>27920300-ca2b-56d2-9a9d-62d03468ac37</td>\n",
       "      <td>[0.03380979, -0.0031218985, 0.0017484669, 0.01...</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>sSJGCjuU2B0XJA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>0d3ac5ba-6f5b-5148-b459-2b8d885d960d</td>\n",
       "      <td>[-0.00075231574, 0.0042315223, 0.0025023425, -...</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721574737.3454354</td>\n",
       "      <td>R4iYKU3EiXOphw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id__  \\\n",
       "0    57b64c23-fc08-5929-a438-f37e3712a73d   \n",
       "1    74f4a51f-0227-5cd1-ac4e-5d2ed8d471e2   \n",
       "2    ac330ebb-e555-5b5a-bee1-cc323af4e931   \n",
       "3    9351a81d-6c25-5223-b0f4-9ee047a620dc   \n",
       "4    fd61d778-c390-5d82-89e4-f77d0b27c8d9   \n",
       "..                                    ...   \n",
       "943  06e95ae8-c6c8-51d6-bb0d-a7c3051f5a05   \n",
       "944  38f191e7-9f59-518a-b060-87d670bddb38   \n",
       "945  3b51d912-fcca-57f9-9060-214e604d66d9   \n",
       "946  27920300-ca2b-56d2-9a9d-62d03468ac37   \n",
       "947  0d3ac5ba-6f5b-5148-b459-2b8d885d960d   \n",
       "\n",
       "                                              vector__  \\\n",
       "0    [-0.00035094196, -0.062014297, -0.037999876, 0...   \n",
       "1    [0.020011412, -0.011535538, 0.013017209, -0.00...   \n",
       "2    [0.014857555, -0.06664993, -0.013571247, 0.023...   \n",
       "3    [-0.023312032, -0.09461493, 0.056361612, -0.00...   \n",
       "4    [0.026537651, -0.01779666, 0.0021155947, 0.006...   \n",
       "..                                                 ...   \n",
       "943  [0.016619362, -0.033603165, -0.093347155, -0.0...   \n",
       "944  [0.026872855, -0.0019949432, 0.008369081, -0.0...   \n",
       "945  [0.03513756, 0.056265566, 0.024428478, -0.0651...   \n",
       "946  [0.03380979, -0.0031218985, 0.0017484669, 0.01...   \n",
       "947  [-0.00075231574, 0.0042315223, 0.0025023425, -...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The purpose of this document is to capture fre...   \n",
       "1    GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2    Yes, even if you don't register, you're still ...   \n",
       "3    You don't need it. You're accepted. You can al...   \n",
       "4    You can start by installing and setting up all...   \n",
       "..                                                 ...   \n",
       "943  Problem description\\nThis is the step in the c...   \n",
       "944  Problem description\\nWhen a docker-compose fil...   \n",
       "945  Problem description\\nIf you are having problem...   \n",
       "946  Problem description\\nPre-commit command was fa...   \n",
       "947  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                              section  \\\n",
       "0    General course-related questions   \n",
       "1    General course-related questions   \n",
       "2    General course-related questions   \n",
       "3    General course-related questions   \n",
       "4    General course-related questions   \n",
       "..                                ...   \n",
       "943          Module 6: Best practices   \n",
       "944          Module 6: Best practices   \n",
       "945          Module 6: Best practices   \n",
       "946          Module 6: Best practices   \n",
       "947          Module 6: Best practices   \n",
       "\n",
       "                                              question        _dlt_load_id  \\\n",
       "0                 Course - When will the course start?  1721574737.3454354   \n",
       "1    Course - What are the prerequisites for this c...  1721574737.3454354   \n",
       "2    Course - Can I still join the course after the...  1721574737.3454354   \n",
       "3    Course - I have registered for the Data Engine...  1721574737.3454354   \n",
       "4     Course - What can I do before the course starts?  1721574737.3454354   \n",
       "..                                                 ...                 ...   \n",
       "943  Github actions: Permission denied error when e...  1721574737.3454354   \n",
       "944  Managing Multiple Docker Containers with docke...  1721574737.3454354   \n",
       "945           AWS regions need to match docker-compose  1721574737.3454354   \n",
       "946                                   Isort Pre-commit  1721574737.3454354   \n",
       "947  How to destroy infrastructure created via GitH...  1721574737.3454354   \n",
       "\n",
       "            _dlt_id  \n",
       "0    Xb2yoE2pdKyqAQ  \n",
       "1    fHsjSKqSkMPknA  \n",
       "2    4qQDkXTBMf+xrQ  \n",
       "3    vIF1WvHmGMbegw  \n",
       "4    tnTBbvaCCvvyEg  \n",
       "..              ...  \n",
       "943  zGKICl/U3GEZow  \n",
       "944  QCcXun8zw6FQhg  \n",
       "945  kcfbZdKZSTP34w  \n",
       "946  sSJGCjuU2B0XJA  \n",
       "947  R4iYKU3EiXOphw  \n",
       "\n",
       "[948 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table(\"qanda_embedded___documents\")\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a50407-3938-44d1-8092-f29829c09a9c",
   "metadata": {},
   "source": [
    "That's all for this intro example! The DB could now be used as a basis for a RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7074bd-468f-4ba1-9e36-054d5a25330e",
   "metadata": {},
   "source": [
    "# Create an up-to-date RAG with dlt and LanceDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab823821-e9fb-40b5-a001-4226dfddd497",
   "metadata": {},
   "source": [
    "In this demo, we will be creating an LLM chat bot that has the latest knowledge of the employee handbook of a fictional company. We will be able to chat to it about specific policies like PTO, work from home etc.\n",
    "\n",
    "To build this, we would need to do three things:\n",
    "1. The company policies exist in a [Notion Page](https://dlthub.notion.site/Employee-handbook-669c2a1e04044465811c8ca22977685d). We will need to first extract the text from these pages.\n",
    "2. Once extracted, we will want to embed them into vectors and then store them in a vector database.\n",
    "3. This will allow us to create our RAG: a function that would accept a user question, match it to the information stored in the vector database, and then send the question + relevant information as input to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886334ff-b15a-46cb-8e37-78b4992e1531",
   "metadata": {},
   "source": [
    "We will be using the following OSS tools for this:\n",
    "1. dlt for data ingestion:  \n",
    "  1. dlt can easily connect to any REST API source (like Notion)\n",
    "  2. It also has integrations with vector databases, like LanceDB.\n",
    "  3. It also allows to easily plug in functionality like incremental loading.\n",
    "2. LanceDB as a vector database:\n",
    "  1. LanceDB is an open-source vector database that is very easy to use and integrate into python workflows\n",
    "  2. It is in-process and serverless (like DuckDB), which makes querying and retreival very efficient\n",
    "3. Ollama for RAG:\n",
    "  1. Ollama is open-source and allows you to easily run LLMs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b637059-39cf-4510-b7d8-c40df74f3a33",
   "metadata": {},
   "source": [
    "**Note on running this notebook**: We are going to download and use a local Ollama instance for the RAG, so preferably select the **T4 GPU** in the runtime when starting this notebook (Runtime > Change runtime type > Hardware accelerator > T4 GPU).\n",
    "\n",
    "You can also use the default CPU in case you're facing technical issues, but then your LLM responses might be slower (~2 mins/response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e78b8-d7aa-4e6f-89c1-a33ac7a3b028",
   "metadata": {},
   "source": [
    "## Part 1: Create a Notion -> LanceDB pipeline using dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d5834-866d-4494-9d36-d149f8b812e6",
   "metadata": {},
   "source": [
    "### 1. Install requirements\n",
    "To create a notion -> lancedb pipeline, we need to install:\n",
    "1. dlt with lancedb extras\n",
    "2. sentence-transformers: we need to use an embedding model to vectorize and store data inside LanceDB. For this we choose the open-source model \"sentence-transformers/all-MiniLM-L6-v2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db37a7ea-5100-4bf3-9b00-9a6ca88486df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install dlt[lancedb]==0.5.1a0\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0512ff8-e197-49ae-8389-659c0b58d2a0",
   "metadata": {},
   "source": [
    "### 2. Create a dlt project with rest_api source and lancedb destination\n",
    "We now create a dlt project using the command `dlt init <source> <destination>`.\n",
    "This downloads all the modules required for the dlt source (rest api, in this case) into the local directory. See the side panel for the directory structure created.\n",
    "What is the dlt rest api source?\n",
    "\n",
    "It is a dlt source that allows you to connect to any REST API endpoint using a declarative configuration. You can:\n",
    "- pass the endpoints that you want to connect to,\n",
    "- define the relation between the endpoints\n",
    "- define how you want to handle pagination and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af3e486-be06-4a0b-9490-35363162e70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up the init scripts in \u001b[1mhttps://github.com/dlt-hub/verified-sources.git\u001b[0m...\n",
      "Cloning and configuring a verified source \u001b[1mrest_api\u001b[0m (Generic API Source)\n",
      "Do you want to proceed? [Y/n]: \n",
      "Verified source \u001b[1mrest_api\u001b[0m was added to your project!\n",
      "* See the usage examples and code snippets to copy from \u001b[1mrest_api_pipeline.py\u001b[0m\n",
      "* Add credentials for \u001b[1mlancedb\u001b[0m and other secrets in \u001b[1m./.dlt/secrets.toml\u001b[0m\n",
      "* \u001b[1mrequirements.txt\u001b[0m was created. Install it with:\n",
      "pip3 install -r requirements.txt\n",
      "* Read \u001b[1mhttps://dlthub.com/docs/walkthroughs/create-a-pipeline\u001b[0m for more information\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | dlt init rest_api lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bddeff7-e6f8-474a-9027-91165daf7880",
   "metadata": {},
   "source": [
    "### 3. Add API credentials\n",
    "To access APIs, databases, or any third-party applications, one might need to specify relevant credentials.\n",
    "\n",
    "With dlt, we can do it in two ways:\n",
    "1. Pass the credentials and any other sensitive information inside `.dlt/secrets.toml`\n",
    "  ```toml\n",
    "  [sources.rest_api.notion]\n",
    "  api_key = \"notion api key\"\n",
    "\n",
    "  [destination.lancedb]\n",
    "  embedding_model_provider = \"sentence-transformers\"\n",
    "  embedding_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "  [destination.lancedb.credentials]\n",
    "  uri = \".lancedb\"\n",
    "  api_key = \"api_key\"\n",
    "  embedding_model_provider_api_key = \"embedding_model_provider_api_key\"\n",
    "  ```\n",
    "2. Pass them as environment variables\n",
    "  ```python\n",
    "  import os\n",
    "  \n",
    "  os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = \"notion api key\"\n",
    "\n",
    "  os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "  os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "  os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\"\n",
    "  os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__API_KEY\"] = \"api_key\"\n",
    "  os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__EMBEDDING_MODEL_PROVIDER_API_KEY\"] = \"embedding_model_provider_api_key\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0808a1b-6bc8-43d3-941b-a958cb813da2",
   "metadata": {},
   "source": [
    "We are going to be using option 2. It's not advisable to paste sensitive information like API keys inside the code, so instead we're going to include them inside the secrets tab in the side panel of the notebook. This will allow us to access the secret values from the notebook.\n",
    "\n",
    "Since we are using the OSS version of LanceDB and OSS embedding models, we only need to specify the API key for Notion.\n",
    "\n",
    "**Note**: You will need to copy the [notion API key](https://share.1password.com/s#da9KgMwPaZUaey3WCaD7ICJoyHDGd3Xos2EZ29WrSWQ) into the secrets tab under the name `SOURCES__REST_API__NOTION__API_KEY`. Make sure to enable notebook access after pasting the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93c94ac-2638-4860-8a48-713b50ed1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from google.colab import userdata\n",
    "\n",
    "# os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde68b7-fb6d-47c3-8862-80eaf20694b1",
   "metadata": {},
   "source": [
    "### 4. Write the pipeline code\n",
    "**Note**: We first go over the code step by step before putting it into runnable cells\n",
    "\n",
    "1. Import necessary modules (run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cd274dc-6ddc-419a-9a39-3534438f8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from rest_api import RESTAPIConfig, rest_api_source\n",
    "\n",
    "from dlt.sources.helpers.rest_client.paginators import BasePaginator, JSONResponsePaginator\n",
    "from dlt.sources.helpers.requests import Response, Request\n",
    "\n",
    "from dlt.destinations.adapters import lancedb_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2852c2-2518-43f3-9c7f-de73b977ffcd",
   "metadata": {},
   "source": [
    "2. Configure the dlt rest api source to connect to and extract the relevant data out from the Notion REST API.\n",
    "\n",
    "  Our notion space has multiple pages and each page has multiple paragraphs (called blocks). To extract all this data from the Notion API, we would first need to get a list of all the page_ids (each page has a unique page_id), and then use the page_id to request the contents from the individual pages. Specifically:\n",
    "  1. We will first request the page_ids from the `/search` endpoint\n",
    "  2. And then using the returned page_ids, we will request the contents from the `/blocks/{page_id}/children` endpoint\n",
    "\n",
    "  With this in mind, we can configure the dlt notion rest api source as follows:\n",
    "  ```python\n",
    "  RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"workshop\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "    Explanation:\n",
    "    1. `client`: Here we added our base url, headers, and authentication\n",
    "    2. `resources`: This is a list of endpoints that we wish to request data from (here: `/search` and `/blocks/{page_id}/children`)\n",
    "    3. [`/search`](https://developers.notion.com/reference/post-search) endpoint:\n",
    "      - The Notion API search endpoint allows us to filter pages based on the title. We can specify which pages we want returned based on the parameter \"query\". For example, if we'd like to return only those pages which has the word \"workshop\" in the title, then we would set `\"query\": \"workshop\"` in the json body.    \n",
    "      - As a response, it returns only page metadata (like page_id). Example response:\n",
    "      ```json\n",
    "          {\n",
    "            \"object\": \"list\",\n",
    "            \"results\": [\n",
    "              {\n",
    "                \"object\": \"page\",\n",
    "                \"id\": \"954b67f9-3f87-41db-8874-23b92bbd31ee\",\n",
    "                \"created_time\": \"2022-07-06T19:30:00.000Z\",\n",
    "                \"last_edited_time\": \"2022-07-06T19:30:00.000Z\",\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "            ],\n",
    "            \"next_cursor\": null,\n",
    "            \"has_more\": false,\n",
    "            \"type\": \"page_or_database\",\n",
    "            \"page_or_database\": {}\n",
    "          }\n",
    "      ```\n",
    "      - This is how we would define our endpoint configuration for `/search`:\n",
    "      ```python\n",
    "           {\n",
    "             \"name\": \"search\",\n",
    "             \"endpoint\": {\n",
    "                 \"path\": \"search\",\n",
    "                 \"method\": \"POST\",\n",
    "                 \"paginator\": PostBodyPaginator(),\n",
    "                 \"json\": {\n",
    "                     \"query\": \"workshop\",\n",
    "                     \"sort\": {\n",
    "                         \"direction\": \"ascending\",\n",
    "                         \"timestamp\": \"last_edited_time\"\n",
    "                     }\n",
    "                 },\n",
    "                 \"data_selector\": \"results\"\n",
    "             }\n",
    "         },\n",
    "      ```\n",
    "      - `paginator` allows us to specify the pagination strategy relevant for the API and the endpoint. (More on this later)\n",
    "      - Since `/search` is a POST endpoint, we can include the json body inside the key `json`.\n",
    "      - We don't need the whole JSON response, but only the contents inside the field \"results\". We filter this out by specifying `\"data_selector\": \"results\"`.\n",
    "    4. [`blocks/{page_id}/children`](https://developers.notion.com/reference/get-block-children) endpoint:\n",
    "      - This is a GET point that returns a list of block objects (in our case, paragraphs) from a specific page.\n",
    "      - Since it accepts page_id as a parameter, we can pass this inside the key `params`\n",
    "      - We would like to be able to automatically fetch the page_ids returned from the `/search` endpoint and pass it as parameter into the endpoint `blocks/{page_id}/children`. We can do this by linking the two resources as follows:\n",
    "      ```python\n",
    "      {\n",
    "            \"name\": \"page_content\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"blocks/{page_id}/children\",\n",
    "                \"paginator\": JSONResponsePaginator(),\n",
    "                \"params\": {\n",
    "                    \"page_id\": {\n",
    "                        \"type\": \"resolve\",\n",
    "                        \"resource\": \"search\",\n",
    "                        \"field\": \"id\"\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "      }\n",
    "      ```\n",
    "      - By specifying `\"type\":\"resolve\"`, we are letting dlt know that this parameter needs to be resolved from the parent resource `\"search\"` using the field `\"id\"`, which corresponds to the page id in the response of `/search`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a1327-dc73-4be4-9bc1-f396d1ebe3fd",
   "metadata": {},
   "source": [
    "  Note on pagination:\n",
    "\n",
    "  Different REST APIs might use different strategies to handle paginated responses. dlt has built-in support for [most common pagination mechanisms](https://dlthub.com/docs/general-usage/http/rest-client#paginators), and these can be explicity passed inside the configuration like shown above.\n",
    "\n",
    "  However in most cases, it won't be necessary to explicity specify the pagination strategy, since dlt detects it automatically.\n",
    "\n",
    "  In case the specific pagination is not supported by dlt yet, then you can also implement a custom paginator. For example, dlt does not have a built-in paginator for POST methods, so we write our own paginator. We take the [code provided in the docs for it](https://dlthub.com/docs/general-usage/http/rest-client#example-2-creating-a-paginator-for-post-requests), and make small modifications to it based on the [notion API documentation](https://developers.notion.com/reference/intro#parameters-for-paginated-requests).\n",
    "\n",
    "  ```python\n",
    "  class PostBodyPaginator(BasePaginator):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.cursor = None\n",
    "\n",
    "      def update_state(self, response: Response) -> None:\n",
    "          # Assuming the API returns an empty list when no more data is available\n",
    "          if not response.json():\n",
    "              self._has_next_page = False\n",
    "          else:\n",
    "              self.cursor = response.json().get(\"next_cursor\")\n",
    "              if self.cursor is None:\n",
    "                  self._has_next_page = False\n",
    "\n",
    "      def update_request(self, request: Request) -> None:\n",
    "          if request.json is None:\n",
    "              request.json = {}\n",
    "\n",
    "          # Add the cursor to the request body\n",
    "          request.json[\"start_cursor\"] = self.cursor\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea169fc3-b38c-4ccb-b749-7e48707af504",
   "metadata": {},
   "source": [
    "3. Extract relevant content from the response body\n",
    "\n",
    "  The response returned from the API is a nested JSON which we need to pre-process before using it anywhere. dlt can unnest json automatically, but since the Notion API is a little tricky it's better to pre-process this first so we have a more structured DB as a result.\n",
    "  \n",
    "  One way to do this is to pass the JSON response through a transformation function that will extract only the relevant data from the JSON body (we later add this as a mapping to the resource):\n",
    "\n",
    "  ```python\n",
    "  def extract_page_content(response):\n",
    "      block_id = response[\"id\"]\n",
    "      last_edited_time = response[\"last_edited_time\"]\n",
    "      block_type = response.get(\"type\", \"Not paragraph\")\n",
    "      if block_type != \"paragraph\":\n",
    "          content = \"\"\n",
    "      else:\n",
    "          try:\n",
    "              content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "          except IndexError:\n",
    "              content = \"\"\n",
    "      return {\n",
    "          \"block_id\": block_id,\n",
    "          \"block_type\": block_type,\n",
    "          \"content\": content,\n",
    "          \"last_edited_time\": last_edited_time,\n",
    "          \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "      }\n",
    "  ```\n",
    "  This is also where you could implement some sort of chunking strategy, but we will omit this in this example as the Notion text is already pre-chunked into paragraphs. Any data pre-processing can also happen here.\n",
    "\n",
    "  **Note**: If you want to include the parent page in the returned data, you can do so by including `response[\"parent\"][\"page_id\"]`. See 200 response example in the [Notion docs](https://developers.notion.com/reference/get-block-children).\n",
    "  \n",
    "  JSON response before the function:\n",
    "  ```\n",
    "  {\n",
    "      \"object\": \"list\",\n",
    "      \"results\": [\n",
    "        {\n",
    "          \"object\": \"block\",\n",
    "          \"id\": \"c02fc1d3-db8b-45c5-a222-27595b15aea7\",\n",
    "          \"created_time\": \"2022-03-01T19:05:00.000Z\",\n",
    "          \"last_edited_time\": \"2022-03-01T19:05:00.000Z\",\n",
    "          .\n",
    "          .\n",
    "          .\n",
    "          \"type\": \"paragraph\",\n",
    "          \"paragraph\": {\n",
    "            \"rich_text\": [\n",
    "              {\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                \"annotations\": {\n",
    "                  .\n",
    "                  .\n",
    "                  .\n",
    "\n",
    "                },\n",
    "                \"plain_text\": \"Lacinato kale is a variety of kale with a long tradition in Italian cuisine, especially that of Tuscany. It is also known as Tuscan kale, Italian kale, dinosaur kale, kale, flat back kale, palm tree kale, or black Tuscan palm.\",\n",
    "                \"href\": \"https://en.wikipedia.org/wiki/Lacinato_kale\"\n",
    "              }\n",
    "            ],\n",
    "            \"color\": \"default\"\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"next_cursor\": null,\n",
    "      \"has_more\": false,\n",
    "      \"type\": \"block\",\n",
    "      \"block\": {}\n",
    "  }\n",
    "  ```\n",
    "  After passing it through the transformation function:\n",
    "\n",
    "  ```\n",
    "  {\n",
    "      \"block_id\": \"c02fc1d3-db8b-45c5-a222-27595b15aea7\",\n",
    "      \"block_type\": \"paragraph\",\n",
    "      \"content\": \"Lacinato kale is a variety of kale with a long tradition in Italian cuisine, especially that of Tuscany. It is also known as Tuscan kale, Italian kale, dinosaur kale, kale, flat back kale, palm tree kale, or black Tuscan palm.\",\n",
    "      \"last_edited_time\": \"2022-03-01T19:05:00.000Z\",\n",
    "  }\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0bb1d-e629-485b-8f93-afa4608b5c7d",
   "metadata": {},
   "source": [
    "4. Load the data incrementally\n",
    "\n",
    "  Incremental loading is a very important aspect of building scalable data pipelines. It is the technique of loading only the new or changed data since the last run of the pipeline.\n",
    "\n",
    "  In our case, when we first run the pipeline, all paragraphs from the employee handbook will get loaded as separate rows inside a lancedb table. Now if we change the content of one of the paragraphs and re-run the pipeline to update the table, then without doing incremental loading, one of two things may happen depending on the option we choose:\n",
    "  - If we choose the option \"replace\", then the existing data in lancedb will be dropped, and all of the paragraphs will be reloaded.\n",
    "  - If we choose the option \"append\", then the existing rows would remain and all of the paragraphs would be loaded again as new rows resulting in twice as many rows\n",
    "\n",
    "  To ensure that only the new/changed rows are loaded we would need the following pieces:\n",
    "  - A column that can keep track of changes in the row (Example: only load rows where `last_edited_time` is greater than the current maximum `last_edited_time`)\n",
    "  - A primary_key column that uniquely identify a row, so it's possible to track when the row changes\n",
    "  - A strategy to resolve changes in a single row (example: drop current and load the changed row)\n",
    "\n",
    "  This behaviour can be configured easily into a dlt rersource:\n",
    "  - Pass the incremental column as a parameter inside the resource\n",
    "    ```python\n",
    "    def rest_api_notion_incremental(\n",
    "      last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "    ):\n",
    "    ```\n",
    "    We choose the column `last_edited_time` since it keeps track of whenever a paragraph changes.\n",
    "  - Pass the following arguments inside `@dlt.resource` to define the strategy for dealing with duplicate rows:\n",
    "    - `write_disposition=\"merge\"`: ensures that any duplicate rows are merged on the primary key\n",
    "    - `primary_key=\"block_id\"`: specifies the primary key that we'd like to merge on. In our case, this is `block_id`, which is a unique id corresponding to each block (paragraph).\n",
    "    - `columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}`: this specifies the deduplication strategy (how we would like to resolve duplicate rows). Here we chose to keep the row with the largest value of `last_edited_time`.\n",
    "\n",
    "    Putting it together:\n",
    "\n",
    "    ```python\n",
    "    @dlt.resource(\n",
    "        name=\"employee_handbook\",\n",
    "        write_disposition=\"merge\",\n",
    "        primary_key=\"block_id\",\n",
    "        columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "    def rest_api_notion_incremental(\n",
    "        last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "    ):\n",
    "        for block in rest_api_notion_resource.add_map(extract_page_content):   \n",
    "            if not(len(block[\"content\"])):\n",
    "                continue\n",
    "            yield block\n",
    "  ```\n",
    "  Here, `rest_api_notion_resoure` yields the JSON response from the Notion REST API and `extract_page_content` is the transformation function that we pass the JSON response through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949d717-da27-4040-a990-404633668a8a",
   "metadata": {},
   "source": [
    "5. Create the pipeline and run it\n",
    "\n",
    "  With our source configured, we can now define the pipeline and run it.\n",
    "\n",
    "  Normally, to do this we would run  \n",
    "  ```python\n",
    "  pipeline.run(\n",
    "    rest_api_notion_incremental,\n",
    "    table_name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\"\n",
    "  )\n",
    "  ```\n",
    "  and this would load the data into lancedb normally, without creating any embeddings.\n",
    "\n",
    "  However, we can have lancedb automatically create embeddings and load it along with the normal data using dlt's native adapter for lancedb: `lancedb_adapter`. It will use the embedding model that we specified in the credentials.   \n",
    "    \n",
    "  ```python\n",
    "  pipeline.run(\n",
    "    lancedb_adapter(\n",
    "      rest_api_notion_incremental,\n",
    "      embed=\"content\" # The column that we'd like to embed\n",
    "    )\n",
    "    table_name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\"\n",
    "  )\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8aa6a8-f536-4f8d-bdda-ecde912b6069",
   "metadata": {},
   "source": [
    "### 5. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa6bbf4-cb79-46d1-a4dd-cf7856f8f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dlt_pipeline_state\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, {'name': 'state', 'data_type': 'text', 'nullable': False}, {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "employee_handbook\n",
      "[{'name': 'block_id', 'nullable': False, 'primary_key': True, 'data_type': 'text'}, {'name': 'block_type', 'data_type': 'text', 'nullable': True}, {'name': 'content', 'x-lancedb-embed': True, 'data_type': 'text', 'nullable': True}, {'dedup_sort': 'desc', 'name': 'last_edited_time', 'data_type': 'timestamp', 'nullable': True}, {'name': 'inserted_at_time', 'data_type': 'timestamp', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "_dlt_version\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, {'name': 'schema', 'data_type': 'text', 'nullable': False}]\n",
      "_dlt_loads\n",
      "[{'name': 'load_id', 'data_type': 'text', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, {'name': 'status', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}]\n",
      "UPLOAD\n",
      "Pipeline company_policies load step completed in 0.86 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset notion_pages\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x7e1ec703dea0> location to store data\n",
      "Load package 1721576657.4298348 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "@dlt.resource(name=\"employee_handbook\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"workshop\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config,name=\"employee_handbook\")\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "):\n",
    "    # last_value = last_edited_time.last_value\n",
    "    # print(last_value)\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "        # full_refresh=True\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        table_name=\"employee_handbook\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6d790-6e66-4ec1-993e-b7faae9ecb5e",
   "metadata": {},
   "source": [
    "### 6. Visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15fe092f-bbea-404c-b42c-1ce3883c9ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>vector__</th>\n",
       "      <th>block_id</th>\n",
       "      <th>block_type</th>\n",
       "      <th>content</th>\n",
       "      <th>last_edited_time</th>\n",
       "      <th>inserted_at_time</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71e89a85-ae0b-5b68-866b-bd3922ec7548</td>\n",
       "      <td>[-0.058588073, -0.07540443, 0.033775173, 0.009...</td>\n",
       "      <td>c0262981-b5f1-4a57-a91f-2e75f649b86c</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company operates between 9 a.m. to 7 p.m. ...</td>\n",
       "      <td>2024-07-18 14:00:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.970497+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>nHvFErZIklyCZg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a28e913f-761f-5684-8cd5-0d0c49e0338c</td>\n",
       "      <td>[-0.0049689207, -0.0039119613, 0.028705634, 0....</td>\n",
       "      <td>faacf4ec-90be-4e96-b8b9-29b5112bc7ca</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Employees receive [20 days] of Paid Time Off (...</td>\n",
       "      <td>2024-06-26 09:03:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.973796+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>NoLIKDxfd3gImw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a18932d9-1583-5c42-bd0d-0f96738c5e6c</td>\n",
       "      <td>[0.0320609, 0.0242446, 0.008471346, 0.03179070...</td>\n",
       "      <td>e6021a51-f403-4950-80c2-ebff005c7289</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company observes the following holidays: N...</td>\n",
       "      <td>2024-06-26 09:08:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.974093+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>u3M5/lNlnIp09A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93661874-13a2-5a43-bed8-868005dfd5e2</td>\n",
       "      <td>[-0.013155272, 0.008382475, 0.017044408, 0.051...</td>\n",
       "      <td>b8f4cc6d-c28c-4071-9545-caadce5eb37b</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>These holidays are considered “off-days” for m...</td>\n",
       "      <td>2024-06-26 09:09:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.974324+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>Z7agKrfdV/DSIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b220778f-1118-5c22-b614-3bc0fd0a602b</td>\n",
       "      <td>[0.027987475, 0.06734363, 0.039806414, 0.00774...</td>\n",
       "      <td>ea7a1beb-6874-4f41-966d-dc1f80a1f635</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Employees who are unable to work due to illnes...</td>\n",
       "      <td>2024-06-26 09:11:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.974561+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>B/hXdG7sOfDsnw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d0f801ba-d3cc-5252-ad6e-3285662b609c</td>\n",
       "      <td>[0.03252609, 0.008159482, 0.084435634, 0.05564...</td>\n",
       "      <td>bd7a9110-fac5-4270-9493-4039ca67b467</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Losing a loved one is traumatizing. If this ha...</td>\n",
       "      <td>2024-06-26 09:17:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.974734+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>8mj4Aj0tT7xi5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>579b97f9-a5e2-53af-b4f7-efc9ad5105ad</td>\n",
       "      <td>[-0.007314059, 0.014710686, -0.019091194, 0.02...</td>\n",
       "      <td>b1718dee-8c0f-4189-8c75-0e8c7844a501</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>In accordance with German law, we offer a comp...</td>\n",
       "      <td>2024-06-26 09:20:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.974897+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>cTI7QIP4Tet3/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a9083b7e-22cc-5b1f-8040-cb7aa1f72338</td>\n",
       "      <td>[-0.031538416, 0.034259938, -0.027282655, 0.02...</td>\n",
       "      <td>5bfa90c5-461d-406a-9324-a1dd54bad0d5</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>We recognize the vital role that fathers and p...</td>\n",
       "      <td>2024-06-26 09:21:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:17.975055+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>xoepiVU9bKmhLg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6adeb540-d180-5d40-bc84-c40e5c173ea1</td>\n",
       "      <td>[-0.038923826, 0.12081745, 0.046208546, -0.005...</td>\n",
       "      <td>baac0ba4-9b60-450e-8cc1-1e6e2a0fb7d9</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>In this section, we describe what we offer to ...</td>\n",
       "      <td>2024-07-03 17:34:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:18.147795+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>rQItAIkTTt11+w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cffdb1bb-a146-5e90-8fbb-a1d577a2a98e</td>\n",
       "      <td>[-0.07571497, 0.14543605, 0.001152166, -0.0244...</td>\n",
       "      <td>0e429073-6383-4918-8961-fcc66346067f</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>{edited} Employee health is important to us. W...</td>\n",
       "      <td>2024-07-18 17:28:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:18.147995+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>QgQruAKedmNB9Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25cd721d-fd64-517f-9b3b-34e3fad3522e</td>\n",
       "      <td>[-0.10974317, 0.10586079, 0.003290621, -0.0213...</td>\n",
       "      <td>f4e006d7-9b38-49e9-94cf-552beaa75773</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company is dedicated to maintaining a safe...</td>\n",
       "      <td>2024-07-03 17:26:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:18.148195+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>qUc3MC3tSCBdbw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c75b7ef9-96b6-551b-9cdd-795bbe01bb6e</td>\n",
       "      <td>[0.050755586, -0.06461987, 0.0652738, 0.014652...</td>\n",
       "      <td>71618ca5-6c62-4b66-bc0f-3d855e0c4b8b</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>If your job doesn’t require you to be present ...</td>\n",
       "      <td>2024-06-26 08:52:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:18.148361+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>3GmFX7K7pGOAkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7a69c4c0-cd55-5090-903e-facf23eadde5</td>\n",
       "      <td>[0.0005233448, -0.054883398, 0.043573365, -0.0...</td>\n",
       "      <td>cd15aaf5-6cdc-4a13-835c-2181fd7bf81e</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Remote working refers to working from a non-of...</td>\n",
       "      <td>2024-07-03 17:19:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:18.148526+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>0zY2QYYtmd/daA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ff1141dc-88f6-500a-a8c3-c18e37661650</td>\n",
       "      <td>[0.03802628, -0.02150967, 0.0475278, 0.0647069...</td>\n",
       "      <td>a4b2f0c9-e0c8-4b3c-81e7-ef624809977d</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>There are some expenses that we will pay direc...</td>\n",
       "      <td>2024-07-05 22:32:00+00:00</td>\n",
       "      <td>2024-07-21 15:44:18.148688+00:00</td>\n",
       "      <td>1721576657.4298348</td>\n",
       "      <td>STu9d4dGAqHouQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id__  \\\n",
       "0   71e89a85-ae0b-5b68-866b-bd3922ec7548   \n",
       "1   a28e913f-761f-5684-8cd5-0d0c49e0338c   \n",
       "2   a18932d9-1583-5c42-bd0d-0f96738c5e6c   \n",
       "3   93661874-13a2-5a43-bed8-868005dfd5e2   \n",
       "4   b220778f-1118-5c22-b614-3bc0fd0a602b   \n",
       "5   d0f801ba-d3cc-5252-ad6e-3285662b609c   \n",
       "6   579b97f9-a5e2-53af-b4f7-efc9ad5105ad   \n",
       "7   a9083b7e-22cc-5b1f-8040-cb7aa1f72338   \n",
       "8   6adeb540-d180-5d40-bc84-c40e5c173ea1   \n",
       "9   cffdb1bb-a146-5e90-8fbb-a1d577a2a98e   \n",
       "10  25cd721d-fd64-517f-9b3b-34e3fad3522e   \n",
       "11  c75b7ef9-96b6-551b-9cdd-795bbe01bb6e   \n",
       "12  7a69c4c0-cd55-5090-903e-facf23eadde5   \n",
       "13  ff1141dc-88f6-500a-a8c3-c18e37661650   \n",
       "\n",
       "                                             vector__  \\\n",
       "0   [-0.058588073, -0.07540443, 0.033775173, 0.009...   \n",
       "1   [-0.0049689207, -0.0039119613, 0.028705634, 0....   \n",
       "2   [0.0320609, 0.0242446, 0.008471346, 0.03179070...   \n",
       "3   [-0.013155272, 0.008382475, 0.017044408, 0.051...   \n",
       "4   [0.027987475, 0.06734363, 0.039806414, 0.00774...   \n",
       "5   [0.03252609, 0.008159482, 0.084435634, 0.05564...   \n",
       "6   [-0.007314059, 0.014710686, -0.019091194, 0.02...   \n",
       "7   [-0.031538416, 0.034259938, -0.027282655, 0.02...   \n",
       "8   [-0.038923826, 0.12081745, 0.046208546, -0.005...   \n",
       "9   [-0.07571497, 0.14543605, 0.001152166, -0.0244...   \n",
       "10  [-0.10974317, 0.10586079, 0.003290621, -0.0213...   \n",
       "11  [0.050755586, -0.06461987, 0.0652738, 0.014652...   \n",
       "12  [0.0005233448, -0.054883398, 0.043573365, -0.0...   \n",
       "13  [0.03802628, -0.02150967, 0.0475278, 0.0647069...   \n",
       "\n",
       "                                block_id block_type  \\\n",
       "0   c0262981-b5f1-4a57-a91f-2e75f649b86c  paragraph   \n",
       "1   faacf4ec-90be-4e96-b8b9-29b5112bc7ca  paragraph   \n",
       "2   e6021a51-f403-4950-80c2-ebff005c7289  paragraph   \n",
       "3   b8f4cc6d-c28c-4071-9545-caadce5eb37b  paragraph   \n",
       "4   ea7a1beb-6874-4f41-966d-dc1f80a1f635  paragraph   \n",
       "5   bd7a9110-fac5-4270-9493-4039ca67b467  paragraph   \n",
       "6   b1718dee-8c0f-4189-8c75-0e8c7844a501  paragraph   \n",
       "7   5bfa90c5-461d-406a-9324-a1dd54bad0d5  paragraph   \n",
       "8   baac0ba4-9b60-450e-8cc1-1e6e2a0fb7d9  paragraph   \n",
       "9   0e429073-6383-4918-8961-fcc66346067f  paragraph   \n",
       "10  f4e006d7-9b38-49e9-94cf-552beaa75773  paragraph   \n",
       "11  71618ca5-6c62-4b66-bc0f-3d855e0c4b8b  paragraph   \n",
       "12  cd15aaf5-6cdc-4a13-835c-2181fd7bf81e  paragraph   \n",
       "13  a4b2f0c9-e0c8-4b3c-81e7-ef624809977d  paragraph   \n",
       "\n",
       "                                              content  \\\n",
       "0   Our company operates between 9 a.m. to 7 p.m. ...   \n",
       "1   Employees receive [20 days] of Paid Time Off (...   \n",
       "2   Our company observes the following holidays: N...   \n",
       "3   These holidays are considered “off-days” for m...   \n",
       "4   Employees who are unable to work due to illnes...   \n",
       "5   Losing a loved one is traumatizing. If this ha...   \n",
       "6   In accordance with German law, we offer a comp...   \n",
       "7   We recognize the vital role that fathers and p...   \n",
       "8   In this section, we describe what we offer to ...   \n",
       "9   {edited} Employee health is important to us. W...   \n",
       "10  Our company is dedicated to maintaining a safe...   \n",
       "11  If your job doesn’t require you to be present ...   \n",
       "12  Remote working refers to working from a non-of...   \n",
       "13  There are some expenses that we will pay direc...   \n",
       "\n",
       "            last_edited_time                 inserted_at_time  \\\n",
       "0  2024-07-18 14:00:00+00:00 2024-07-21 15:44:17.970497+00:00   \n",
       "1  2024-06-26 09:03:00+00:00 2024-07-21 15:44:17.973796+00:00   \n",
       "2  2024-06-26 09:08:00+00:00 2024-07-21 15:44:17.974093+00:00   \n",
       "3  2024-06-26 09:09:00+00:00 2024-07-21 15:44:17.974324+00:00   \n",
       "4  2024-06-26 09:11:00+00:00 2024-07-21 15:44:17.974561+00:00   \n",
       "5  2024-06-26 09:17:00+00:00 2024-07-21 15:44:17.974734+00:00   \n",
       "6  2024-06-26 09:20:00+00:00 2024-07-21 15:44:17.974897+00:00   \n",
       "7  2024-06-26 09:21:00+00:00 2024-07-21 15:44:17.975055+00:00   \n",
       "8  2024-07-03 17:34:00+00:00 2024-07-21 15:44:18.147795+00:00   \n",
       "9  2024-07-18 17:28:00+00:00 2024-07-21 15:44:18.147995+00:00   \n",
       "10 2024-07-03 17:26:00+00:00 2024-07-21 15:44:18.148195+00:00   \n",
       "11 2024-06-26 08:52:00+00:00 2024-07-21 15:44:18.148361+00:00   \n",
       "12 2024-07-03 17:19:00+00:00 2024-07-21 15:44:18.148526+00:00   \n",
       "13 2024-07-05 22:32:00+00:00 2024-07-21 15:44:18.148688+00:00   \n",
       "\n",
       "          _dlt_load_id         _dlt_id  \n",
       "0   1721576657.4298348  nHvFErZIklyCZg  \n",
       "1   1721576657.4298348  NoLIKDxfd3gImw  \n",
       "2   1721576657.4298348  u3M5/lNlnIp09A  \n",
       "3   1721576657.4298348  Z7agKrfdV/DSIA  \n",
       "4   1721576657.4298348  B/hXdG7sOfDsnw  \n",
       "5   1721576657.4298348  8mj4Aj0tT7xi5g  \n",
       "6   1721576657.4298348  cTI7QIP4Tet3/A  \n",
       "7   1721576657.4298348  xoepiVU9bKmhLg  \n",
       "8   1721576657.4298348  rQItAIkTTt11+w  \n",
       "9   1721576657.4298348  QgQruAKedmNB9Q  \n",
       "10  1721576657.4298348  qUc3MC3tSCBdbw  \n",
       "11  1721576657.4298348  3GmFX7K7pGOAkg  \n",
       "12  1721576657.4298348  0zY2QYYtmd/daA  \n",
       "13  1721576657.4298348  STu9d4dGAqHouQ  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\".lancedb\")\n",
    "dbtable = db.open_table(\"notion_pages___employee_handbook\")\n",
    "\n",
    "dbtable.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151fcd9-b38e-477c-ac1d-5995b20fb351",
   "metadata": {},
   "source": [
    "Question 1. Rows in LanceDB (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1cf69bc-bc7c-4f5a-9d01-95a05028cbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dbtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323b99a-bbf1-4335-9f3e-a6e4e108ab06",
   "metadata": {},
   "source": [
    "Question 2. Running the Pipeline: Last edited time (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07b046a4-5712-489b-aa5b-d05440504dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LanceTable' object has no attribute 'last_edited_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdbtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_edited_time\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LanceTable' object has no attribute 'last_edited_time'"
     ]
    }
   ],
   "source": [
    "dbtable.last_edited_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98495d14-34d3-49f7-9278-abb2ad10a602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8857866-3421-41c3-adf6-8ceb03c12f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6547f-d251-403c-9f94-17baeb10fd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef809b5c-b588-46b7-8d0f-467a7c4de1ce",
   "metadata": {},
   "source": [
    "Now we make change to one of the paragraphs and run the pipeline again to see the effect of incremental loading. We observe two things:\n",
    "1. The column `inserted_at_time` only changed for the updated row, implying that only this row was added\n",
    "2. Looking at the primary key `block_id` we see that the original row was dropped and the updated row was inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9076772-b222-4310-88f2-70939901a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = lancedb.connect(\".lancedb\")\n",
    "# dbtable = db.open_table(\"notion_pages___employee_handbook\")\n",
    "\n",
    "# dbtable.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499be46-ee4e-4be6-98aa-381fb9da1488",
   "metadata": {},
   "source": [
    "## Part 2: Create a RAG bot using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd44c0-d94f-42f1-bc42-6db6aa68aaac",
   "metadata": {},
   "source": [
    "\n",
    "What is RAG?\n",
    "\n",
    "Retrieval Automated Generation (RAG) is the framework of retrieving relevant documents from a database and passing it along with a query into an LLM so that the LLM can generate context-aware responses.\n",
    "\n",
    "In our case, if we were to ask an LLM questions about our specific employee policies, then we would not get useful responses because the LLM has never seen these policies. A solution to this could be to paste all of the policies into the prompt and then ask our questions. However, this would not be feasible given the limitations on the size of the context window.\n",
    "\n",
    "We can bypass this limitation using RAG:\n",
    "1. Given a user question, we would first embed this question into a vector\n",
    "2. Then we would do a vector search on our LanceDB table and retrieve top k results - which would be the most relevant paragraphs corresponding to the question\n",
    "3. Finally, we would pass the original question along with the retrieved paragraphs as a prompt into the LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc97619-356e-4684-949e-03b5e5146e92",
   "metadata": {},
   "source": [
    "1. Install Ollama into the notebook's local runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818178d5-1d2e-4cdb-91ed-38682122871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Downloading ollama...\n",
      "######################################################################## 100.0%#=#=#                                                                         \n",
      ">>> Installing ollama to /usr/local/bin...\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c3f98-d448-480b-a848-8806b65be5b1",
   "metadata": {},
   "source": [
    "3. Pull the desired model. We're going to be using `llama1-uncensored` (takes about 1m to download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f67ccf-f6d7-4f40-b18f-3a25a2b2c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!ollama pull llama2-uncensored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f6300-cefa-4781-afa9-ca1a0394f7c9",
   "metadata": {},
   "source": [
    "4. pip install ollama and import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b964570-bf9b-4a03-b4e0-cb258e48b69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/codespace/.local/lib/python3.10/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.12.2)\n",
      "Downloading ollama-0.3.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a3e2e27-70a4-419e-94ad-7bcc296b6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e4591-9705-48fc-9cf8-1ec27f963350",
   "metadata": {},
   "source": [
    "5. Write a function that can retrieve content from lancedb relevant to the user query\n",
    "  \n",
    "  With LanceDB, you don't have to explicity embed the question. LanceDB stores information on the embedding model used and automatically embeds the question.\n",
    "\n",
    "  We use the `db_table.search()` function to query the DB and then limit it to the top 2 most similar results and return that as the context to pass to the RAG.\n",
    "\n",
    "  Limiting results is important because otherwise there might be too much confusing information. Similarly only picking the top choice might not give enough information.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04ddca84-0822-4856-a07e-5c513c7534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_from_lancedb(dbtable, question, top_k=2):\n",
    "\n",
    "    query_results = dbtable.search(query=question).to_list()\n",
    "    context = \"\\n\".join([result[\"content\"] for result in query_results[:top_k]])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2da6c6-7dc5-4822-a156-192e67b0f3d0",
   "metadata": {},
   "source": [
    "6. Finally we define a very basic RAG. We define a simple system prompt, retrieve the relevant context for the user query with the function defined above and then send the user question and the context to the `llama2-uncensored` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43ab9f7c-50db-43da-8e82-453095e04b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Connect to the lancedb table\n",
    "  db = lancedb.connect(\".lancedb\")\n",
    "  dbtable = db.open_table(\"notion_pages___employee_handbook\")\n",
    "\n",
    "  # A system prompt telling ollama to accept input in the form of \"Question: ... ; Context: ...\"\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps users understand policies inside a company's employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\"}\n",
    "  ]\n",
    "\n",
    "  while True:\n",
    "    # Accept user question\n",
    "    question = input(\"You: \")\n",
    "\n",
    "    # Retrieve the relevant paragraphs on the question\n",
    "    context = retrieve_context_from_lancedb(dbtable,question,top_k=2)\n",
    "\n",
    "    # Create a user prompt using the question and retrieved context\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": f\"Question: '{question}'; Context:'{context}'\"}\n",
    "    )\n",
    "\n",
    "    # Get the response from the LLM\n",
    "    response = ollama.chat(\n",
    "        model=\"llama2-uncensored\",\n",
    "        messages=messages\n",
    "    )\n",
    "    response_content = response['message']['content']\n",
    "    print(f\"Assistant: {response_content}\")\n",
    "\n",
    "    # Add the response into the context window\n",
    "    messages.append(\n",
    "        {\"role\": \"assistant\", \"content\":response_content}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35402c-52b7-4e51-95b5-e06643699280",
   "metadata": {},
   "source": [
    "And we run the RAG! Some example questions you can ask:\n",
    "\n",
    "* How many vacation days do I get?\n",
    "* Can I get maternity leave?\n",
    "\n",
    "**Note**: This is a very basic implementation of a RAG, since this workshop is mainly about data ingestion. So expect some weird answers. If you do stop and restart the cell, you will need to rerun the cell containing `ollama serve` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1298f0f0-5225-49be-92fc-7c118e3f16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How many vacation days do I get?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     20\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; Context:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get the response from the LLM\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama2-uncensored\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m response_content \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/ollama/_client.py:235\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    233\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/ollama/_client.py:98\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     93\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     95\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     97\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 98\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/ollama/_client.py:69\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 69\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d74fdf-4dcd-44e4-a0b2-8554439b7288",
   "metadata": {},
   "source": [
    "There's a lot more to learn and do with dlt and LanceDB, find more info the [dlt docs](https://dlthub.com/docs/) and the [LanceDB docs](https://lancedb.github.io/lancedb/)\n",
    "\n",
    "If you have questions about this workshop or dlt, feel free to join our [community on Slack](https://dlthub.com/community).\n",
    "\n",
    "If you're at EuroPython in Prague this week, come see us at our booth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016710be-34a3-4351-95e2-91efa648f52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c003c6-163a-4334-b294-bdbff7f8814d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726fcc-b27d-4dce-8bfb-55fc96f9efb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46788f5d-40ed-45be-9ef5-de16bc9353da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b39003-8708-4b4c-a2f9-5e2694970a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea0f22-393a-4779-a1cb-5f93237f25eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3a3cf-8796-48df-acc1-ceef8b774b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029b295-a23c-49ce-887d-3b9525891416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a10b9a-85ed-4b3f-8e40-41cf43ca3db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875529bb-435f-4fb6-8cce-1dd5421e4fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
